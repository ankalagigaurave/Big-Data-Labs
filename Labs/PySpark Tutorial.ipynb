{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "PySpark Fundamentals-checkpoint.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankalagigaurave/Big-Data-Labs/blob/main/Labs/PySpark%20Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mslPKC9fEEkQ"
      },
      "source": [
        "## Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LPPSPKFEEkX"
      },
      "source": [
        "import findspark\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from functools import reduce"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0piV91YEEkY"
      },
      "source": [
        "## PySpark SQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z6O0i_sEEkY"
      },
      "source": [
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk9CgZ1TEEkY",
        "outputId": "21d02819-f22e-48ee-c299-fa9d2597e775"
      },
      "source": [
        "spark = SparkSession.builder.getOrCreate()\n",
        "df = spark.sql(\"select 'spark' as hello\")\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|spark|\n",
            "+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L38dOracEEkZ"
      },
      "source": [
        "### PySpark Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlNfJPgdEEka"
      },
      "source": [
        "```pyspark.SparkContext(master = None, appName = None, sparkHome = None, pyFiles = None, environment = None, \n",
        "                     batchSize = 0, serializer = PickleSerializer(), conf = None, gateway = None, jsc = None,\n",
        "                     profiler_cls = <class 'pyspark.profiler.BasicProfiler'>)```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPrphScUEEka",
        "outputId": "bc1c65e6-8c1a-46b3-fb21-b7d1352c93e3"
      },
      "source": [
        "conf = SparkConf().setAppName(\"RatingsHistogram\").setMaster(\"local\")\n",
        "\n",
        "# Making The Spark Context\n",
        "sc = SparkContext.getOrCreate(conf = conf)\n",
        "sc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://192.168.1.6:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ7nH2_gEEkb"
      },
      "source": [
        "### Stopping The Pyspark Context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O3zmVOdEEkb"
      },
      "source": [
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVylSZT_EEkb",
        "outputId": "8f412ed8-9a4c-498c-d737-1f40f698702b"
      },
      "source": [
        "x= ['Python', 'is', 'really', 'conveniant']\n",
        "print(sorted(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Python', 'conveniant', 'is', 'really']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjgmuHnlEEkc",
        "outputId": "40d6faf1-d60e-4f13-b5a3-e90cd0c7125c"
      },
      "source": [
        "sorted(x, key = lambda arg : arg.lower())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['conveniant', 'is', 'Python', 'really']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br9KqWnMEEkc"
      },
      "source": [
        "## Filtering()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIqlz-qiEEkc",
        "outputId": "25c07423-9823-4eea-cf15-09671c9aed1a"
      },
      "source": [
        "list(filter(lambda arg: len(arg) < 5, x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['is']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUTBjEyPEEkd",
        "outputId": "b8c2ac58-cc7f-453a-a237-11f93ece9418"
      },
      "source": [
        "def check_length(x):\n",
        "    return len(x)<5\n",
        "\n",
        "l1=[]\n",
        "for i in x:\n",
        "    if check_length(i):\n",
        "        l1.append(i)   \n",
        "print(l1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['is']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-_AKVWxEEkd"
      },
      "source": [
        "## Map()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2tDxBfYEEkd",
        "outputId": "b673466c-de93-468f-e4fe-ce371728420b"
      },
      "source": [
        "print(list(map(lambda arg : check_length(arg), x)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[False, True, False, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMNgB9bUEEke",
        "outputId": "d2ef462b-1654-4a00-ac72-245c80b6c376"
      },
      "source": [
        "def check_length(x):\n",
        "    return len(x)<5\n",
        "\n",
        "l1=[]\n",
        "for i in x:\n",
        "    l1.append(check_length(i))   \n",
        "print(l1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[False, True, False, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzINOEyPEEke"
      },
      "source": [
        "## Reduce()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYujyLE5EEke"
      },
      "source": [
        "def concat(x,y):\n",
        "    print( f'{x} + {y} = {x+y}')\n",
        "    return x + y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jTsb9PEEEke"
      },
      "source": [
        "*Python + is<br>\n",
        "Pythonis + really<br>\n",
        "Pythonisreally + conveniant<br>\n",
        "Pythonisreallyconveniant<br>*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl4kfy9UEEke",
        "outputId": "de36f221-2505-4bc7-b793-e714ef0acd99"
      },
      "source": [
        "print(reduce(lambda i, j: concat(i, j), x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python + is = Pythonis\n",
            "Pythonis + really = Pythonisreally\n",
            "Pythonisreally + conveniant = Pythonisreallyconveniant\n",
            "Pythonisreallyconveniant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRmlC3IqEEkf",
        "outputId": "50452d2b-b331-427f-98e1-25f6855c56b2"
      },
      "source": [
        "print(reduce(lambda i, j: concat(i, j), range(10)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 + 1 = 1\n",
            "1 + 2 = 3\n",
            "3 + 3 = 6\n",
            "6 + 4 = 10\n",
            "10 + 5 = 15\n",
            "15 + 6 = 21\n",
            "21 + 7 = 28\n",
            "28 + 8 = 36\n",
            "36 + 9 = 45\n",
            "45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCsQxs66EEkf"
      },
      "source": [
        "## PySpark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0prRcbedEEkf",
        "outputId": "89e55ba9-c341-4157-91fe-ab1bef6567d4"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import pyspark\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
        "sc = pyspark.SparkContext('local[*]')\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/Users/nmims/Desktop/Semester VI/Big Data Technologies/Big-Data-Lab-Sem-VI'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUdBFxUSEEkf",
        "outputId": "7399c576-9372-4f6d-99ba-2c436a7455c2"
      },
      "source": [
        "txt = sc.textFile('file:///C:/Users/Gaurav Ankalagi/Desktop/Gaurav Ankalagi/Big Data/test')\n",
        "print(txt.count())\n",
        "\n",
        "python_lines = txt.filter(lambda line: 'spark' in line.lower())\n",
        "print(python_lines.count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48\n",
            "19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO3TkgrEEEkg"
      },
      "source": [
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1sD20V1EEkg"
      },
      "source": [
        "## RDD Resilient Distributed Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHmdaSw4EEkg",
        "outputId": "0d2e9cae-d336-4345-a632-89105d7383ee"
      },
      "source": [
        "sc = pyspark.SparkContext('local[*]')\n",
        "\n",
        "words = sc.parallelize(['scala', 'mahout', 'solaris', 'vertica', 'reddis', 'hadoop', 'lunaris'])\n",
        "words.count()\n",
        "words.collect()\n",
        "\n",
        "def test(x):\n",
        "    print(x)\n",
        "\n",
        "test_result = words.foreach(test)\n",
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by getOrCreate at <ipython-input-3-6799541de75a>:1 ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-5e372413d2be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local[*]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scala'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mahout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'solaris'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vertica'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reddis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hadoop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lunaris'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    343\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by getOrCreate at <ipython-input-3-6799541de75a>:1 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwMziy3WEEkg"
      },
      "source": [
        "sc = pyspark.SparkContext('local[*]', 'cache check')\n",
        "words = sc.parallelize(['scala', 'mahout', 'solaris', 'vertica', 'reddis', 'hadoop', 'lunaris'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ0VpuUDEEkh",
        "outputId": "7fa41b73-5777-4bd9-f7ba-db3520551218"
      },
      "source": [
        "words.persist().is_cached"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo8Kw4egEEkh",
        "outputId": "d6dfb3cf-6cfd-46b0-edb5-997fd8d102ef"
      },
      "source": [
        "words.cache()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8yt5B5NEEkh"
      },
      "source": [
        "## Shared Variables\n",
        "\n",
        "* Broadcast\n",
        "* Accumulator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StXYdwX0EEkh",
        "outputId": "3236bd55-e5a8-4b0f-8acb-86198c3c31d4"
      },
      "source": [
        "words_2 = sc.broadcast(['scala', 'mahout', 'solaris', 'vertica', 'reddis', 'hadoop', 'lunaris'])\n",
        "data = words_2.value\n",
        "print(f'Stored data is {data}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0fcf0c923a07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scala'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mahout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'solaris'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vertica'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reddis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hadoop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lunaris'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Stored data is {data}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNUfVqE1EEki",
        "outputId": "fb57771e-fa8d-4978-a795-52cf5e4fb31f"
      },
      "source": [
        "words_2.value[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'solaris'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d043lyeAEEki",
        "outputId": "6a0ec63a-3c5d-41af-8938-8a8072e7bf24"
      },
      "source": [
        "y = sc.accumulator(100)\n",
        "def f(x):\n",
        "    global y\n",
        "    y+=x\n",
        "    \n",
        "rdd = sc.parallelize([10,20,30,40,50])\n",
        "rdd.foreach(f)\n",
        "\n",
        "final = y.value\n",
        "print(final)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsN8GrlxEEki"
      },
      "source": [
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiiuGGQVEEkj"
      },
      "source": [
        "## Spark mllib\n",
        "\n",
        "* classification\n",
        "* regression\n",
        "* clustering\n",
        "* linalg\n",
        "* recommendation\n",
        "* fpm"
      ]
    }
  ]
}